{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP7/9DdgT9dm5LzjjyVXpEg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sapan-s2/AI_Austin_course/blob/main/Baging_Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHvc2MT4pbRy"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading dataset\n",
        "data=pd.read_csv(\"/content/Cars-dataset.csv\")"
      ],
      "metadata": {
        "id": "Ib3fr7C1q2jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "aiq9BUbtq8Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "uyKMSPUYq9AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "T7yPnaDErAVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()\n"
      ],
      "metadata": {
        "id": "poEeN0y7rDDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.nunique()"
      ],
      "metadata": {
        "id": "gP9qxoVNsp9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(16,10)})\n",
        "sns.heatmap(data.corr(),\n",
        "            annot=True,\n",
        "            linewidths=.5,\n",
        "            center=0,\n",
        "            cbar=False,\n",
        "            cmap=\"Spectral\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bpvjx1L2suuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create dummies for gender col\n",
        "\n",
        "data['Gender'] = data['Gender'].astype('category')\n",
        "data = pd.get_dummies(data, columns=['Gender'])\n"
      ],
      "metadata": {
        "id": "SBgYZQk2uapR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Split the data into a 70:30 ratio\n",
        "\n",
        "X = data.drop(['Opt_service'], axis=1)\n",
        "y = data['Opt_service']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "s7pKnJWruhdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: What is the percentage of 0 and 1 classes in the test data (y_test)?\n",
        "\n",
        "print(y_test.value_counts()/len(y_test)*100)\n"
      ],
      "metadata": {
        "id": "qhgjbZ4iuqnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.value_counts()/len(y_test)"
      ],
      "metadata": {
        "id": "402DSsKyvP4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Build a bagging classifier with default parameters\n",
        "\n",
        "#Fitting the model\n",
        "bagging_classifier = BaggingClassifier(random_state=1)\n",
        "bagging_classifier.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "id": "GLIPnl8lvduN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
        "def model_performance_classification_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred)  # to compute Recall\n",
        "    precision = precision_score(target, pred)  # to compute Precision\n",
        "    f1 = f1_score(target, pred)  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\n",
        "            \"Accuracy\": acc,\n",
        "            \"Recall\": recall,\n",
        "            \"Precision\": precision,\n",
        "            \"F1\": f1,\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ],
      "metadata": {
        "id": "lfF9NtSKwmhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    To plot the confusion_matrix with percentages\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(predictors)\n",
        "    cm = confusion_matrix(target, y_pred)\n",
        "    labels = np.asarray(\n",
        "        [\n",
        "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
        "            for item in cm.flatten()\n",
        "        ]\n",
        "    ).reshape(2, 2)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")\n"
      ],
      "metadata": {
        "id": "z7564RL8wp5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Libtune to tune model, get different metric scores\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "94jh6q0cwb2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating different metrics\n",
        "bagging_classifier_model_train_perf=model_performance_classification_sklearn(bagging_classifier,X_train,y_train)\n",
        "print(\"Training performance:\\n\",bagging_classifier_model_train_perf)\n",
        "bagging_classifier_model_test_perf=model_performance_classification_sklearn(bagging_classifier,X_test,y_test)\n",
        "print(\"Testing performance:\\n\",bagging_classifier_model_test_perf)\n",
        "#Creating confusion matrix\n",
        "confusion_matrix_sklearn(bagging_classifier, X_test, y_test)"
      ],
      "metadata": {
        "id": "LW-HYQF_wNg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: How many employees who would take the service are correctly identified by the model from the training data\n",
        "\n",
        "bagging_classifier_model_train_perf['Recall']\n"
      ],
      "metadata": {
        "id": "UvIxH5Shw2nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "metadata": {
        "id": "L302iMV7yR9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import logisticregression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "metadata": {
        "id": "EPSCnKhgybFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Build a random forest classifier with default parameters and a bagging classifier with logistic regression as the base estimator\n",
        "\n",
        "random_forest_classifier = RandomForestClassifier(random_state=1)\n",
        "bagging_classifier_l = BaggingClassifier(base_estimator=LogisticRegression(),random_state=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "rzhTS0-bw94_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bagging_classifier_l.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "NV81nefBy7O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "random_forest_classifier.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "jxH1hgt2z_j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating different metrics\n",
        "bagging_classifier_model_train_perf_l=model_performance_classification_sklearn(bagging_classifier_l,X_train,y_train)\n",
        "print(\"Training performance:\\n\",bagging_classifier_model_train_perf_l)\n",
        "# bagging_classifier_model_test_perf_l=model_performance_classification_sklearn(bagging_classifier_l,X_test,y_test)\n",
        "# print(\"Testing performance:\\n\",bagging_classifier_model_test_perf_l)\n",
        "# #Creating confusion matrix\n",
        "# confusion_matrix_sklearn(bagging_classifier_l, X_test, y_test)"
      ],
      "metadata": {
        "id": "KXMEIDLuyh_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating different metrics\n",
        "random_forest_classifier_model_train_perf_l=model_performance_classification_sklearn(random_forest_classifier,X_train,y_train)\n",
        "print(\"Training performance:\\n\",random_forest_classifier_model_train_perf_l)\n",
        "# random_forest_classifier_model_test_perf_l=model_performance_classification_sklearn(random_forest_classifier,X_test,y_test)\n",
        "# print(\"Testing performance:\\n\",random_forest_classifier_model_test_perf_l)\n",
        "# #Creating confusion matrix\n",
        "# confusion_matrix_sklearn(bagging_classifier_l, X_test, y_test)"
      ],
      "metadata": {
        "id": "ac6rr3K-y9uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: bagging classifier model with the base estimator as a decision tree.Vary the depth of the base estimator/Decision tree from depth 1 to 5\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a BaggingClassifier with a DecisionTreeClassifier as the base estimator\n",
        "bagging_classifier_d = BaggingClassifier(base_estimator=DecisionTreeClassifier())\n",
        "\n",
        "# Create a grid search object to find the optimal depth for the DecisionTreeClassifier\n",
        "grid_search = GridSearchCV(bagging_classifier_d, param_grid={'base_estimator__max_depth': range(1, 6)}, scoring='f1')\n",
        "\n",
        "# Fit the grid search object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by the grid search\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Print the accuracy score of the best model on the test data\n",
        "best_model = grid_search.best_estimator_\n",
        "accuracy = accuracy_score(y_test, best_model.predict(X_test))\n",
        "print(accuracy)\n"
      ],
      "metadata": {
        "id": "6WbxQGKX0oIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "for i in range(1,6):\n",
        " bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=1,max_depth = i),random_state=1)\n",
        " bag.fit(X_train, y_train)\n",
        " pred = bag.predict(X_train)\n",
        " case = {'Depth':i,'F1 Score':f1_score(y_train,pred)}\n",
        " scores.append(case)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "CRPtk2My94IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import XGBoostClassifier\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "id": "mne5rkSK2AZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Which of the boosting models (AdaBoost, GradientBoost, XGBoost) with default parameters has the lowest f1-score on the training set. Set the eval_metric = 'logloss' for XGBoostClassifier\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Create models\n",
        "ada_boost = AdaBoostClassifier(random_state=1)\n",
        "grad_boost = GradientBoostingClassifier(random_state=1)\n",
        "\n",
        "xgb_boost=XGBRegressor(random_state=1,eval_metric='logloss')\n",
        "# xgb_estimator.fit(X_train,y_train)\n",
        "\n",
        "# xgb_boost = XGBoostClassifier(eval_metric='logloss')\n",
        "\n",
        "# Fit models\n",
        "ada_boost.fit(X_train, y_train)\n",
        "grad_boost.fit(X_train, y_train)\n",
        "xgb_boost.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate models\n",
        "ada_boost_f1 = f1_score(y_train, ada_boost.predict(X_train))\n",
        "print(\"ada_boost_f1\", ada_boost_f1)\n",
        "grad_boost_f1 = f1_score(y_train, grad_boost.predict(X_train))\n",
        "print(\"grad_boost_f1\", grad_boost_f1)\n",
        "xgb_boost_f1 = f1_score(y_train, xgb_boost.predict(X_train).round())\n",
        "\n",
        "# Find model with lowest f1-score\n",
        "lowest_f1_model = min([ada_boost_f1, grad_boost_f1])\n",
        "\n",
        "# Print model with lowest f1-score\n",
        "print(f'Model with lowest f1-score on training set: {lowest_f1_model}')\n"
      ],
      "metadata": {
        "id": "PtAyFI2u1wRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: f1 score for xgboost\n",
        "\n",
        "xgb_boost_f1 = f1_score(y_train, xgb_boost.predict(X_train))\n"
      ],
      "metadata": {
        "id": "pb5nlESf3pLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Use the gradient boosting classifier trained in Q6  and plot the feature importance of the variable\n",
        "\n",
        "# feature_importances = xgb_boost.feature_importances_\n",
        "# # Make a sorted list of feature importances\n",
        "# sorted_importances = sorted(feature_importances, reverse=True)\n",
        "# # Create a bar chart of the feature importances\n",
        "# plt.bar(range(len(sorted_importances)), sorted_importances)\n",
        "# plt.xlabel(\"Feature\")\n",
        "# plt.ylabel(\"Importance\")\n",
        "# plt.title(\"Feature Importance\")\n",
        "# plt.show()\n",
        "\n",
        "feature_names = X_train.columns\n",
        "importances = xgb_boost.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UkLXRMAY4FB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Train three models:  1. Model1 = Gradient Boosting classifier with n_estimator = 50 and learning rate = 0.01 2.  Model2 = Gradient Boosting classifier with n_estimator = 100 and learning rate = 0.01 3. Model3 = Gradient Boosting classifier with n_estimator = 400 and learning rate = 0.01  f1_score1, f1_score2, f1_score3 are f1_scores of the three models respectively.  order of f1_score on the training set\n",
        "\n",
        "model1 = GradientBoostingClassifier(n_estimators=50, learning_rate=0.01)\n",
        "model1.fit(X_train, y_train)\n",
        "f1_score1 = f1_score(y_train, model1.predict(X_train))\n",
        "\n",
        "model2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01)\n",
        "model2.fit(X_train, y_train)\n",
        "f1_score2 = f1_score(y_train, model2.predict(X_train))\n",
        "\n",
        "model3 = GradientBoostingClassifier(n_estimators=400, learning_rate=0.01)\n",
        "model3.fit(X_train, y_train)\n",
        "f1_score3 = f1_score(y_train, model3.predict(X_train))\n",
        "\n",
        "print(f\"f1_score1: {f1_score1}\")\n",
        "print(f\"f1_score2: {f1_score2}\")\n",
        "print(f\"f1_score3: {f1_score3}\")\n",
        "\n",
        "if f1_score1 > f1_score2 and f1_score1 > f1_score3:\n",
        "  print(\"f1_score1 is the highest\")\n",
        "elif f1_score2 > f1_score1 and f1_score2 > f1_score3:\n",
        "  print(\"f1_score2 is the highest\")\n",
        "else:\n",
        "  print(\"f1_score3 is the highest\")\n"
      ],
      "metadata": {
        "id": "xyTtTGIP4l4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ".98 > 0.0"
      ],
      "metadata": {
        "id": "szL4sYPq5ftZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: What is the order of f1_score on the training set in above q?\n",
        "\n",
        "xgb_boost_f1 = f1_score(y_train, xgb_boost.predict(X_train).round())\n",
        "grad_boost_f1 = f1_score(y_train, grad_boost.predict(X_train))\n",
        "ada_boost_f1 = f1_score(y_train, ada_boost.predict(X_train))\n",
        "\n",
        "# Print the f1-scores in order from highest to lowest\n",
        "print(f'f1-scores on training set in order from highest to lowest:')\n",
        "print(f'XGBoost: {xgb_boost_f1}')\n",
        "print(f'Gradient Boosting: {grad_boost_f1}')\n",
        "print(f'AdaBoost: {ada_boost_f1}')\n"
      ],
      "metadata": {
        "id": "1zXgMiQ746lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Build a stacking classifier using two models - Decision Tree, Bagging Classifier as base estimators and use Random Forest as the final estimator\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Create a Bagging Classifier using Decision Tree as the base estimator\n",
        "bagging_classifier = BaggingClassifier(base_estimator=dt_classifier,random_state=1)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=1)\n",
        "\n",
        "# Create a StackingClassifier using Decision Tree and Bagging Classifier as base estimators and Random Forest as the final estimator\n",
        "stacking_classifier = StackingClassifier(estimators=[('dt_classifier', dt_classifier), ('bagging_classifier', bagging_classifier)], final_estimator=rf_classifier)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,  random_state=1)\n",
        "\n",
        "# Fit the stacking classifier to the training data\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = stacking_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the stacking classifier\n",
        "# accuracy = accuracy_score(y_test, stacking_classifier.predict(X_train) )\n",
        "# print('Accuracy:', accuracy)\n",
        "\n",
        "print(f1_score(y_train, stacking_classifier.predict(X_train)))\n",
        "print(recall_score(y_train, stacking_classifier.predict(X_train)))\n"
      ],
      "metadata": {
        "id": "ihk1z0ev5lag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Stacking classifier f1 score and recall score on training set\n",
        "\n",
        "print(f1_score(y_train, stacking_classifier.predict(X_train)))\n",
        "print(recall_score(y_train, stacking_classifier.predict(X_train)))\n"
      ],
      "metadata": {
        "id": "_kSWWG0G7xMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Build a stacking classifier using two models - AdaBoost classifier, and Gradient Boosting Classifier as base estimators and use XGBoost as the final estimator. random_state=1, f1 and recall score on test set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score, recall_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Create the base models\n",
        "ada_boost = AdaBoostClassifier(random_state=1)\n",
        "grad_boost = GradientBoostingClassifier(random_state=1)\n",
        "\n",
        "# Create the stacking classifier\n",
        "stacking_classifier = StackingClassifier(estimators=[('ada_boost', ada_boost), ('grad_boost', grad_boost)], final_estimator=XGBClassifier(random_state=1))\n",
        "\n",
        "# Fit the stacking classifier to the training data\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = stacking_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the stacking classifier\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(f\"f1_score: {f1}\")\n",
        "print(f\"recall_score: {recall}\")\n"
      ],
      "metadata": {
        "id": "L2HkdNSH8s9r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}